{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader ,Dataset\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2D_f = pd.read_csv(\"../Data/Microbiome_data/T2D_data/T2D_fuctional.csv\",sep=\",\")\n",
    "T2D_tax = pd.read_csv(\"../Data/Microbiome_data/T2D_data/T2D_taxo.csv\",sep=\",\")\n",
    "T2D_gen = pd.read_csv(\"../Data/Microbiome_data/T2D_data/T2D_gen40.csv\",sep=\",\")\n",
    "T2D_labels = pd.read_csv(\"../Data/Microbiome_data/T2D_data/T2D_ylab.txt\")\n",
    "T2D_labels= T2D_labels.iloc[:-1]\n",
    "T2D_labels = T2D_labels[:].values\n",
    "T2D_labels= pd.DataFrame(T2D_labels)\n",
    "T2D_labels = T2D_labels.replace({\"Control\": 1, \"T2D\": 0})\n",
    "\n",
    "CRC_f = pd.read_csv(\"../Data/Microbiome_data/CRC_data/CRC_Fuctional.csv\",sep=\",\")\n",
    "CRC_tax = pd.read_csv(\"../Data/Microbiome_data/CRC_data/CRC_Taxo.csv\",sep=\",\")\n",
    "CRC_gen = pd.read_csv(\"../Data/Microbiome_data/CRC_data/CRC_gen40.csv\",sep=\",\")\n",
    "CRC_labels = pd.read_csv(\"../Data/Microbiome_data/CRC_data/CRC_ylab.txt\")\n",
    "CRC_labels= CRC_labels.iloc[:-1]\n",
    "CRC_labels = CRC_labels[:].values\n",
    "CRC_labels= pd.DataFrame(CRC_labels)\n",
    "CRC_labels = CRC_labels.replace({\"control\": 1, \"CRC\": 2})\n",
    "\n",
    "IBD_f = pd.read_csv(\"../Data/Microbiome_data/IBD_data/IBD_fuctional.csv\",sep=\",\")\n",
    "IBD_tax = pd.read_csv(\"../Data/Microbiome_data/IBD_data/IBD_taxo.csv\",sep=\",\")\n",
    "IBD_gen = pd.read_csv(\"../Data/Microbiome_data/IBD_data/IBD_gen40.csv\",sep=\",\")\n",
    "IBD_labels = pd.read_csv(\"../Data/Microbiome_data/IBD_data/IBD_ylab.txt\")\n",
    "IBD_labels= IBD_labels.iloc[:-1]\n",
    "IBD_labels = IBD_labels[:].values\n",
    "IBD_labels= pd.DataFrame(IBD_labels)\n",
    "IBD_labels = IBD_labels.replace({\"Normal\": 1, \"IBD\": 3})\n",
    "\n",
    "\n",
    "LC_f = pd.read_csv(\"../Data/Microbiome_data/LC_data/LC_Fuctional.csv\",sep=\",\")\n",
    "LC_tax = pd.read_csv(\"../Data/Microbiome_data/LC_data/LC_taxo.csv\",sep=\",\")\n",
    "LC_gen = pd.read_csv(\"../Data/Microbiome_data/LC_data/LC_gen40.csv\",sep=\",\")\n",
    "LC_labels = pd.read_csv(\"../Data/Microbiome_data/LC_data/LC_ylab.txt\")\n",
    "LC_labels= LC_labels.iloc[:-1]\n",
    "LC_labels = LC_labels[:].values\n",
    "LC_labels= pd.DataFrame(LC_labels)\n",
    "LC_labels = LC_labels.replace({\"Normal\": 1, \"Cirrhosis\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = torch.tensor(T2D_f.values)\n",
    "f2 = torch.tensor(T2D_gen.values)\n",
    "f3 = torch.tensor(T2D_tax.values)\n",
    "f4 = torch.tensor(IBD_f.values)\n",
    "f5 = torch.tensor(IBD_gen.values)\n",
    "f6 = torch.tensor(IBD_tax.values)\n",
    "f7= torch.tensor(CRC_f.values)\n",
    "f8 = torch.tensor(CRC_gen.values)\n",
    "f9= torch.tensor(CRC_tax.values)\n",
    "f10 = torch.tensor(LC_f.values)\n",
    "f11= torch.tensor(LC_gen.values)\n",
    "f12 = torch.tensor(LC_tax.values)\n",
    "\n",
    "l1 = torch.tensor(T2D_labels.values)\n",
    "l2 = torch.tensor(IBD_labels.values)\n",
    "l3 = torch.tensor(CRC_labels.values)\n",
    "l4 = torch.tensor(LC_labels.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2D_comb = torch.cat((f1,f2,f3), dim=1)\n",
    "IBD_comb = torch.cat((f4,f5,f6), dim=1)\n",
    "CRC_comb = torch.cat((f7,f8,f9), dim=1)\n",
    "LC_comb = torch.cat((f10,f11,f12), dim=1)\n",
    "compined = torch.cat((T2D_comb,IBD_comb,CRC_comb,LC_comb))\n",
    "\n",
    "labels_combined = torch.cat((l1,l2,l3,l4))\n",
    "\n",
    "Test1 = torch.cat((IBD_comb,CRC_comb))\n",
    "TestL= torch.cat((l2,l3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save it to tensor .pt objects\n",
    "torch.save(labels_combined, '../Data/Tensors_objs/labels.pt')\n",
    "torch.save(compined, '../Data/Tensors_objs/features.pt')\n",
    "np.savetxt(\"test\",Test1,delimiter =\",\")\n",
    "np.savetxt(\"TestL\",TestL,delimiter=\",\")\n",
    "np.savetxt(\"Data\", compined, delimiter=\",\")\n",
    "np.savetxt(\"Labels\", labels_combined, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load your data into a numpy array called X\n",
    "X = np.loadtxt(\"test\", delimiter=\",\")\n",
    "y = np.loadtxt(\"TestL\")\n",
    "\n",
    "# Separate your class labels from your feature data\n",
    "\n",
    "\n",
    "# Apply PCA to reduce the dimensionality of your data to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot your data in 2 dimensions, colored by class label\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.legend(y)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = nn.functional.relu(self.fc1(x))\n",
    "        print(h1.shape)\n",
    "        mu, logvar = self.fc21(h1), self.fc22(h1)\n",
    "        print(mu.shape)\n",
    "        print(logvar.shape)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = nn.functional.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.reshape(-1, self.input_dim).float())\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim, num_classes):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim + num_classes, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim , hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x, y):\n",
    "        # Concatenate input and target data\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        print(x.shape)\n",
    "        \n",
    "        # Pass through encoder network\n",
    "        x = self.activation(self.fc1(x))\n",
    "        #print(x.dtype)\n",
    "        mu = self.fc21(x)\n",
    "        #print(mu.dtype)\n",
    "        logvar = self.fc22(x)\n",
    "        #print(logvar.dtype)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Sample from the reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        #print(std.shape)\n",
    "        eps = torch.randn_like(std)\n",
    "        #print(eps.shape)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z, y):\n",
    "        # Concate target / latent \n",
    "        z = torch.cat([z, y], dim=1)\n",
    "        print(z.shape)\n",
    "        # Pass through decoder network\n",
    "        z = self.activation(self.fc3(z))\n",
    "        recon = self.fc4(z)\n",
    "        #print(recon.shape)\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, logvar = self.encode(x, y)\n",
    "        #print(mu.shape)\n",
    "        #print(logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #print(z.shape)\n",
    "        recon = self.decode(z, y)\n",
    "        #print(recon.shape)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def multiclass_vae_loss(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss\n",
    "    BCE = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    recon_loss = BCE(recon_x, x) / x.size(0)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD /= x.size(0) * x.size(1)\n",
    "    \n",
    "    return recon_loss + KLD\n",
    "\n",
    "def train_multiclass_vae(model, train_loader, optimizer, num_epochs):\n",
    "    # Train the VAE for num_epochs epochs\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(data.float(),   target.float())\n",
    "            print(recon.shape)\n",
    "            print(mu.shape)\n",
    "            print(logvar.shape)\n",
    "            loss = multiclass_vae_loss(recon, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    print('Epoch {}, Loss: {:.4f}'.format(epoch+1, total_loss / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the microbiome data and labels into PyTorch tensors\n",
    "data = torch.load(\"../Data/Tensors_objs/features.pt\")\n",
    "labels = torch.load(\"../Data/Tensors_objs/labels.pt\")\n",
    "data = torch.DoubleTensor(data)\n",
    "labels = labels.float()\n",
    "labels = labels.double()\n",
    "#labels = torch.DoubleTensor(labels)\n",
    "#nan_indices = torch.isnan(data)\n",
    "#data = torch.where(nan_indices, torch.tensor(0.0), data)\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(data, labels)\n",
    "# Define the dataset and dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([636, 150])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 151])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x151 and 15x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [385], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m VAE(\u001b[39m10\u001b[39m,\u001b[39m2\u001b[39m, \u001b[39m10\u001b[39m,\u001b[39m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train_multiclass_vae(model,dataloader,optimizer,\u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [367], line 83\u001b[0m, in \u001b[0;36mtrain_multiclass_vae\u001b[0;34m(model, train_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     82\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 83\u001b[0m     recon, mu, logvar \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mfloat(),   target\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     84\u001b[0m     \u001b[39mprint\u001b[39m(recon\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(mu\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [367], line 56\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[0;32m---> 56\u001b[0m     mu, logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x, y)\n\u001b[1;32m     57\u001b[0m     \u001b[39m#print(mu.shape)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[39m#print(logvar.shape)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(mu, logvar)\n",
      "Cell \u001b[0;32mIn [367], line 28\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Pass through encoder network\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     29\u001b[0m \u001b[39m#print(x.dtype)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m mu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc21(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x151 and 15x10)"
     ]
    }
   ],
   "source": [
    "model = VAE(10,2, 10,5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "train_multiclass_vae(model,dataloader,optimizer,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_size*2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_size)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=-1)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps*std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_hat, mu, log_var):\n",
    "    # Reconstruction error\n",
    "    recon_loss = F.mse_loss(x_hat, x, reduction='sum')\n",
    "\n",
    "    # KL divergence\n",
    "    kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = (recon_loss + kld_loss)\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [396], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m x \u001b[39m=\u001b[39m batch_data\n\u001b[0;32m---> 19\u001b[0m x_hat, mu, log_var \u001b[39m=\u001b[39m vae(x)\n\u001b[1;32m     21\u001b[0m loss \u001b[39m=\u001b[39m vae_loss(x, x_hat, mu, log_var)\n\u001b[1;32m     22\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [394], line 41\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 41\u001b[0m     mu, log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x)\n\u001b[1;32m     42\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(mu, log_var)\n\u001b[1;32m     43\u001b[0m     x_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z)\n",
      "Cell \u001b[0;32mIn [394], line 26\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 26\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     27\u001b[0m     mu, log_var \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(h, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m mu, log_var\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "# Define the VAE model\n",
    "input_size = 16 # size of each input feature\n",
    "latent_size =2 # size of the latent variables\n",
    "vae = VAE(input_size, latent_size)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50# number of epochs\n",
    "batch_size =128 # size of each batch\n",
    "data_loader =  dataloader # torch.utils.data.DataLoader for your dataset\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch_data in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = batch_data\n",
    "        x_hat, mu, log_var = vae(x)\n",
    "\n",
    "        loss = vae_loss(x, x_hat, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, total_loss / len(data_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.cross_entropy(recon_x, x.reshape(-1, input_dim), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae1(model, eex, epochs, batch_size, learning_rate):\n",
    "    train_loader =  DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, eex in enumerate(train_loader):\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data.float())\n",
    "            loss = loss_function(recon_batch, data.float(), mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Epoch: {} \\t Loss: {:.6f}'.format(epoch+1, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 150 # Define the size of your input data\n",
    "my_list = 2\n",
    "z_dim = 150 # Define the size of the latent space\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(input_dim,my_list, z_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the microbiome data and labels into PyTorch tensors\n",
    "data = torch.load(\"../Data/Tensors_objs/features.pt\")\n",
    "labels = torch.load(\"../Data/Tensors_objs/labels.pt\")\n",
    "data = torch.DoubleTensor(data)\n",
    "labels = labels.float()\n",
    "labels = labels.double()\n",
    "#labels = torch.DoubleTensor(labels)\n",
    "nan_indices = torch.isnan(data)\n",
    "data = torch.where(nan_indices, torch.tensor(0.0), data)\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(data, labels)\n",
    "# Define the dataset and dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VAE.forward() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [386], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_vae1(model, dataloader, \u001b[39m50\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m1e-4\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [98], line 11\u001b[0m, in \u001b[0;36mtrain_vae1\u001b[0;34m(model, eex, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, eex \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m     recon_batch, mu, logvar \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     12\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(recon_batch, data\u001b[39m.\u001b[39mfloat(), mu, logvar)\n\u001b[1;32m     13\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: VAE.forward() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "train_vae1(model, dataloader, 50, 32, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VAE.state_dict(model), '../Model/vae_one_study2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load Model and encode data\n",
    "VAE.load_state_dict(model, torch.load('vae_one_study2.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     z, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode(data\u001b[39m.\u001b[39mfloat())\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z, _ = model.decode(data.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and classes needed in this example:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from itertools import product\n",
    "\n",
    "# Import Gaussian Naive Bayes classifier:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 0 2 1 0 1 4 4 1 1 2 0 4 0 3 1 3 1 1 1 1 3 1 3 3 4 0 4 1 0 3 4 2 1 1 1\n",
      " 1 2 1 3 1 3 4 2 1 3 1 2 2 3 0 3 3 0 1 1 0 4 1 1 1 1 1 3 1 1 0 1 4 3 1 1 1\n",
      " 1 1 1 2 4 1 0 1 4 4 4 4 2 1 1 1 1 0 3 1 4 1 1 4 4 1 1 0 3 4 4 1 1 4 0 1 1\n",
      " 1 2 1 4 1 0 3 4 3 4 3 0 4 3 2 1 4]\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into random train and test subsets:\n",
    "train, test, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=5)\n",
    "\n",
    "# Initialize classifier:\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier:\n",
    "classifier = gnb.fit(train, train_labels)\n",
    "# Make predictions with the classifier:\n",
    "predictive_labels = gnb.predict(test)\n",
    "print(predictive_labels)\n",
    "\n",
    "# Evaluate label (subsets) accuracy:\n",
    "print(accuracy_score(test_labels, predictive_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 4 3 4 4 3 4 0 1 1 1 1 4 1 1 0 0 0 2 4 1 1 4 1 1 1 1 1 1 1 1 2 4 1 4\n",
      " 1 3 0 1 1 0 0 4 0 3 3 1 2 1 3 1 1 1 4 1 1 0 1 0 1 1 0 1 1 0 1 0 4 0 4 1 2\n",
      " 4 1 1 3 1 1 4 1 1 1 1 4 3 4 1 3 4 3 0 2 3 4 2 3 2 1 1 1 1 2 0 1 1 4 1 1 1\n",
      " 1 3 3 1 1 4 0 1 3 1 1 1 1 2 1 1 1]\n",
      "0.9921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into random train and test subsets:\n",
    "train, test, train_labels, test_labels = train_test_split(compined, labels, test_size=0.2, random_state=11)\n",
    "\n",
    "# Initialize classifier:\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier:\n",
    "classifier = gnb.fit(train, train_labels)\n",
    "# Make predictions with the classifier:\n",
    "predictive_labels = gnb.predict(test)\n",
    "print(predictive_labels)\n",
    "\n",
    "# Evaluate label (subsets) accuracy:\n",
    "print(accuracy_score(test_labels, predictive_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(compined, labels)\n",
    "# Evaluate the classifier on the validation set\n",
    "y_val_pred = clf.predict(test)\n",
    "val_accuracy = accuracy_score(test_labels, y_val_pred)\n",
    "val_confusion_matrix = confusion_matrix(test_labels, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 1.0000\n",
      "Validation confusion matrix:\n",
      "[[17  0  0  0  0]\n",
      " [ 0 66  0  0  0]\n",
      " [ 0  0  9  0  0]\n",
      " [ 0  0  0 15  0]\n",
      " [ 0  0  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation confusion matrix:\\n{val_confusion_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries and data points\n",
    "x_min, x_max = test.min() - 0.1, test.max() + 0.1\n",
    "y_min, y_max = test.min() - 0.1, test.max() + 0.1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first, find the indices of the points in class A and class B\n",
    "indices_A = np.where(labels == 1)[0]\n",
    "indices_B = np.where(labels == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,\n",
       "        55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,\n",
       "        68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,\n",
       "        81,  82,  83,  84,  85,  86,  87,  88, 146, 147, 148, 149, 150,\n",
       "       151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163,\n",
       "       164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176,\n",
       "       177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
       "       190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202,\n",
       "       203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "       216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228,\n",
       "       229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
       "       242, 243, 244, 344, 345, 346, 347, 348, 352, 353, 354, 355, 357,\n",
       "       358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371,\n",
       "       372, 373, 374, 376, 377, 378, 379, 381, 382, 383, 384, 385, 386,\n",
       "       389, 390, 391, 392, 393, 397, 398, 399, 402, 404, 405, 406, 408,\n",
       "       410, 411, 412, 414, 415, 417, 420, 422, 423, 424, 461, 462, 463,\n",
       "       464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
       "       477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "       490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502,\n",
       "       503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515,\n",
       "       516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528,\n",
       "       529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541,\n",
       "       542])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data points (Tensors) in class A and class B\n",
    "points_A = data[indices_A]\n",
    "points_B = data[indices_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  closest point in class B to the first point in class A\n",
    "point_A = points_A[0]\n",
    "distances = torch.norm(points_B - point_A, dim=1)\n",
    "min_distance = torch.min(distances)\n",
    "closest_point_B = points_B[torch.argmin(distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum Euclidean distance between a point in class A and a point in class B is 24.572020023521226.\n",
      "The closest point in class B to the first point in class A is tensor([0.0000, 0.0000, 0.1819, 0.0000, 0.0000, 0.0000, 0.0000, 0.2239, 0.2333,\n",
      "        0.1362, 0.1235, 0.0000, 0.0000, 0.0000, 0.1215, 0.1320, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.1500, 0.0000, 0.0119, 0.0000, 0.0000, 0.0726,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2151, 0.0000, 0.0000, 0.1345,\n",
      "        0.0000, 0.1855, 0.0000, 0.1912, 0.0000, 0.0000, 0.2760, 0.0000, 0.0666,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2519, 0.0000, 0.0000, 1.2691,\n",
      "        0.0000, 0.0000, 0.0000, 2.5403, 0.0000, 1.4951, 1.4841, 1.2158, 3.4579,\n",
      "        0.0000, 0.2574, 1.5570, 0.0000, 3.4918, 1.1940, 0.0000, 0.0000, 0.1771,\n",
      "        0.0000, 0.0000, 1.9311, 1.8469, 4.2595, 2.4102, 1.9308, 0.0000, 1.5183,\n",
      "        4.4092, 0.0000, 0.4729, 0.0000, 0.2926, 2.7506, 0.0000, 0.0000, 2.1808,\n",
      "        6.7957, 3.5056, 2.4417, 0.0000, 0.0000, 0.1032, 0.8042, 0.0991, 0.1461,\n",
      "        3.3183, 0.0000, 1.9539, 1.0659, 0.0775, 0.0000, 0.4768, 0.0000, 0.0270,\n",
      "        0.0000, 1.5593, 3.4844, 1.9633, 0.3306, 0.0000, 0.0000, 0.0000, 1.8488,\n",
      "        0.0000, 1.3271, 0.0000, 0.0000, 0.0000, 0.0000, 1.9618, 0.0000, 3.0954,\n",
      "        1.7719, 0.0000, 2.6623, 1.8815, 0.0000, 0.0000, 0.1827, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.3736, 0.5692, 2.3507, 0.0347, 0.0000, 3.6566, 1.9310,\n",
      "        0.0000, 0.0000, 2.7296, 2.5002, 0.0000, 1.0050], dtype=torch.float64).\n",
      "The difference between the two points is tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -1.9172,  0.0000,  0.0000, -1.8660, -1.4439,  0.0000,\n",
      "         0.0000, -0.4773,  0.0000,  1.4951, -2.9613,  1.2158,  3.4579,  0.0000,\n",
      "         0.2574, -5.1283,  0.0000,  3.2428, -3.5884,  0.0000,  0.0000, -4.5995,\n",
      "         0.0000, -1.2142,  1.9311, -3.1258,  4.2595,  2.4102,  1.9308,  0.0000,\n",
      "        -5.2664,  4.4092,  0.0000,  0.4729,  0.0000, -6.5822,  2.7506,  0.0000,\n",
      "        -4.6088,  2.1808,  6.7957,  3.5056, -3.3284,  0.0000,  0.0000, -2.8530,\n",
      "        -3.6366, -5.2729, -6.1247,  3.1598,  0.0000,  1.8872,  0.9012, -3.4233,\n",
      "        -3.2590, -0.3176,  0.0000, -2.7533, -1.9591,  0.7810,  3.4391,  1.8413,\n",
      "        -1.6831,  0.0000,  0.0000, -2.4847,  1.8488,  0.0000,  1.1764,  0.0000,\n",
      "         0.0000,  0.0000, -1.2193,  1.9618,  0.0000,  2.6741,  1.7719,  0.0000,\n",
      "         2.5285,  0.9513,  0.0000, -1.8964, -2.0705,  0.0000, -1.3737,  0.0000,\n",
      "        -2.2585, -0.9377,  0.3989,  1.8909, -3.2630, -3.2157,  3.6566,  1.5836,\n",
      "         0.0000, -2.3542,  2.6934,  2.2065,  0.0000,  0.9973],\n",
      "       dtype=torch.float64).\n"
     ]
    }
   ],
   "source": [
    "# calculate the difference between the two points\n",
    "difference = closest_point_B - point_A\n",
    "\n",
    "print(f\"The minimum Euclidean distance between a point in class A and a point in class B is {min_distance}.\")\n",
    "print(f\"The closest point in class B to the first point in class A is {closest_point_B}.\")\n",
    "print(f\"The difference between the two points is {difference}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = torch.where(torch.all(data == closest_point_B, axis=1))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.9539, 1.0659, 0.0775, 0.0000, 0.4768, 0.0000, 0.0270, 0.0000,\n",
       "        1.5593, 3.4844, 1.9633, 0.3306, 0.0000, 0.0000, 0.0000, 1.8488, 0.0000,\n",
       "        1.3271, 0.0000, 0.0000, 0.0000, 0.0000, 1.9618, 0.0000, 3.0954, 1.7719,\n",
       "        0.0000, 2.6623, 1.8815, 0.0000, 0.0000, 0.1827, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.3736, 0.5692, 2.3507, 0.0347, 0.0000, 3.6566, 1.9310, 0.0000,\n",
       "        0.0000, 2.7296, 2.5002, 0.0000, 1.0050, 2.2519, 0.0000, 0.0000, 1.2691,\n",
       "        0.0000, 0.0000, 0.0000, 2.5403, 0.0000, 1.4951, 1.4841, 1.2158, 3.4579,\n",
       "        0.0000, 0.2574, 1.5570, 0.0000, 3.4918, 1.1940, 0.0000, 0.0000, 0.1771,\n",
       "        0.0000, 0.0000, 1.9311, 1.8469, 4.2595, 2.4102, 1.9308, 0.0000, 1.5183,\n",
       "        4.4092, 0.0000, 0.4729, 0.0000, 0.2926, 2.7506, 0.0000, 0.0000, 2.1808,\n",
       "        6.7957, 3.5056, 2.4417, 0.0000, 0.0000, 0.1032, 0.8042, 0.0991, 0.1461,\n",
       "        3.3183, 2.2519, 0.0000, 0.0000, 1.2691, 0.0000, 0.0000, 0.0000, 2.5403,\n",
       "        0.0000, 1.4951, 1.4841, 1.2158, 3.4579, 0.0000, 0.2574, 1.5570, 0.0000,\n",
       "        3.4918, 1.1940, 0.0000, 0.0000, 0.1771, 0.0000, 0.0000, 1.9311, 1.8469,\n",
       "        4.2595, 2.4102, 1.9308, 0.0000, 1.5183, 4.4092, 0.0000, 0.4729, 0.0000,\n",
       "        0.2926, 2.7506, 0.0000, 0.0000, 2.1808, 6.7957, 3.5056, 2.4417, 0.0000,\n",
       "        0.0000, 0.1032, 0.8042, 0.0991, 0.1461, 3.3183], dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/pl8gm0gs50d6vpk0nxfh07_h0000gn/T/ipykernel_20804/1601153969.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  single_tensor = torch.tensor(data[99])  # or any other single tensor\n"
     ]
    }
   ],
   "source": [
    "single_tensor = torch.tensor(data[99])  # or any other single tensor\n",
    "decoded_tensor = model.decode(single_tensor.unsqueeze(0).float())  # pass to the decoder of your VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# compute Euclidean distances between decoded tensor and all original tensors\n",
    "distances = cdist(decoded_tensor.detach().numpy(), data.numpy(), metric='euclidean')\n",
    "\n",
    "# find index of the original tensor with the smallest distance\n",
    "index = torch.argmin(torch.from_numpy(distances))\n",
    "\n",
    "# select the original tensor with the smallest distance\n",
    "original_sample = dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(465)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
