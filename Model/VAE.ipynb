{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader ,Dataset\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2D_f = pd.read_csv(\"../Data/Microbiome_data/T2D_data/T2D_fuctional.csv\",sep=\",\")\n",
    "T2D_tax = pd.read_csv(\"../Data/Microbiome_data/T2D_data/T2D_taxo.csv\",sep=\",\")\n",
    "T2D_gen = pd.read_csv(\"../Data/Microbiome_data/T2D_data/T2D_gen40.csv\",sep=\",\")\n",
    "T2D_labels = pd.read_csv(\"../Data/Microbiome_data/T2D_data/T2D_ylab.txt\")\n",
    "T2D_labels= T2D_labels.iloc[:-1]\n",
    "T2D_labels = T2D_labels[:].values\n",
    "T2D_labels= pd.DataFrame(T2D_labels)\n",
    "T2D_labels = T2D_labels.replace({\"Control\": 1, \"T2D\": 0})\n",
    "\n",
    "CRC_f = pd.read_csv(\"../Data/Microbiome_data/CRC_data/CRC_Fuctional.csv\",sep=\",\")\n",
    "CRC_tax = pd.read_csv(\"../Data/Microbiome_data/CRC_data/CRC_Taxo.csv\",sep=\",\")\n",
    "CRC_gen = pd.read_csv(\"../Data/Microbiome_data/CRC_data/CRC_gen40.csv\",sep=\",\")\n",
    "CRC_labels = pd.read_csv(\"../Data/Microbiome_data/CRC_data/CRC_ylab.txt\")\n",
    "CRC_labels= CRC_labels.iloc[:-1]\n",
    "CRC_labels = CRC_labels[:].values\n",
    "CRC_labels= pd.DataFrame(CRC_labels)\n",
    "CRC_labels = CRC_labels.replace({\"control\": 1, \"CRC\": 2})\n",
    "\n",
    "IBD_f = pd.read_csv(\"../Data/Microbiome_data/IBD_data/IBD_fuctional.csv\",sep=\",\")\n",
    "IBD_tax = pd.read_csv(\"../Data/Microbiome_data/IBD_data/IBD_taxo.csv\",sep=\",\")\n",
    "IBD_gen = pd.read_csv(\"../Data/Microbiome_data/IBD_data/IBD_gen40.csv\",sep=\",\")\n",
    "IBD_labels = pd.read_csv(\"../Data/Microbiome_data/IBD_data/IBD_ylab.txt\")\n",
    "IBD_labels= IBD_labels.iloc[:-1]\n",
    "IBD_labels = IBD_labels[:].values\n",
    "IBD_labels= pd.DataFrame(IBD_labels)\n",
    "IBD_labels = IBD_labels.replace({\"Normal\": 1, \"IBD\": 3})\n",
    "\n",
    "\n",
    "LC_f = pd.read_csv(\"../Data/Microbiome_data/LC_data/LC_Fuctional.csv\",sep=\",\")\n",
    "LC_tax = pd.read_csv(\"../Data/Microbiome_data/LC_data/LC_taxo.csv\",sep=\",\")\n",
    "LC_gen = pd.read_csv(\"../Data/Microbiome_data/LC_data/LC_gen40.csv\",sep=\",\")\n",
    "LC_labels = pd.read_csv(\"../Data/Microbiome_data/LC_data/LC_ylab.txt\")\n",
    "LC_labels= LC_labels.iloc[:-1]\n",
    "LC_labels = LC_labels[:].values\n",
    "LC_labels= pd.DataFrame(LC_labels)\n",
    "LC_labels = LC_labels.replace({\"Normal\": 1, \"Cirrhosis\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = torch.tensor(T2D_f.values)\n",
    "f2 = torch.tensor(T2D_gen.values)\n",
    "f3 = torch.tensor(T2D_tax.values)\n",
    "f4 = torch.tensor(IBD_f.values)\n",
    "f5 = torch.tensor(IBD_gen.values)\n",
    "f6 = torch.tensor(IBD_tax.values)\n",
    "f7= torch.tensor(CRC_f.values)\n",
    "f8 = torch.tensor(CRC_gen.values)\n",
    "f9= torch.tensor(CRC_tax.values)\n",
    "f10 = torch.tensor(LC_f.values)\n",
    "f11= torch.tensor(LC_gen.values)\n",
    "f12 = torch.tensor(LC_tax.values)\n",
    "\n",
    "l1 = torch.tensor(T2D_labels.values)\n",
    "l2 = torch.tensor(IBD_labels.values)\n",
    "l3 = torch.tensor(CRC_labels.values)\n",
    "l4 = torch.tensor(LC_labels.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2D_comb = torch.cat((f1,f2,f3), dim=1)\n",
    "IBD_comb = torch.cat((f4,f5,f6), dim=1)\n",
    "CRC_comb = torch.cat((f7,f8,f9), dim=1)\n",
    "LC_comb = torch.cat((f10,f11,f12), dim=1)\n",
    "compined = torch.cat((T2D_comb,IBD_comb,CRC_comb,LC_comb))\n",
    "\n",
    "labels_combined = torch.cat((l1,l2,l3,l4))\n",
    "\n",
    "Test1 = torch.cat((IBD_comb,))\n",
    "TestL= torch.cat((l2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save it to tensor .pt objects\n",
    "torch.save(labels_combined, '../Data/Tensors_objs/labels.pt')\n",
    "torch.save(compined, '../Data/Tensors_objs/features.pt')\n",
    "np.savetxt(\"test\",Test1,delimiter =\",\")\n",
    "np.savetxt(\"TestL\",TestL,delimiter=\",\")\n",
    "np.savetxt(\"Data\", compined, delimiter=\",\")\n",
    "np.savetxt(\"Labels\", labels_combined, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load your data into a numpy array called X\n",
    "X = np.loadtxt(\"test\", delimiter=\",\")\n",
    "y = np.loadtxt(\"TestL\")\n",
    "\n",
    "# Separate your class labels from your feature data\n",
    "\n",
    "\n",
    "# Apply PCA to reduce the dimensionality of your data to 2 dimensions\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot your data in 2 dimensions, colored by class label\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.legend(y)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = nn.functional.relu(self.fc1(x))\n",
    "        print(h1.shape)\n",
    "        mu, logvar = self.fc21(h1), self.fc22(h1)\n",
    "        print(mu.shape)\n",
    "        print(logvar.shape)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = nn.functional.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.reshape(-1, self.input_dim).float())\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim, num_classes):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim + num_classes, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim , hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x, y):\n",
    "        # Concatenate input and target data\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        print(x.shape)\n",
    "        \n",
    "        # Pass through encoder network\n",
    "        x = self.activation(self.fc1(x))\n",
    "        #print(x.dtype)\n",
    "        mu = self.fc21(x)\n",
    "        #print(mu.dtype)\n",
    "        logvar = self.fc22(x)\n",
    "        #print(logvar.dtype)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Sample from the reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        #print(std.shape)\n",
    "        eps = torch.randn_like(std)\n",
    "        #print(eps.shape)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z, y):\n",
    "        # Concate target / latent \n",
    "        z = torch.cat([z, y], dim=1)\n",
    "        print(z.shape)\n",
    "        # Pass through decoder network\n",
    "        z = self.activation(self.fc3(z))\n",
    "        recon = self.fc4(z)\n",
    "        #print(recon.shape)\n",
    "        return recon\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        mu, logvar = self.encode(x, y)\n",
    "        #print(mu.shape)\n",
    "        #print(logvar.shape)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        #print(z.shape)\n",
    "        recon = self.decode(z, y)\n",
    "        #print(recon.shape)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def multiclass_vae_loss(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss\n",
    "    BCE = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    recon_loss = BCE(recon_x, x) / x.size(0)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KLD /= x.size(0) * x.size(1)\n",
    "    \n",
    "    return recon_loss + KLD\n",
    "\n",
    "def train_multiclass_vae(model, train_loader, optimizer, num_epochs):\n",
    "    # Train the VAE for num_epochs epochs\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(data.float(),   target.float())\n",
    "            print(recon.shape)\n",
    "            print(mu.shape)\n",
    "            print(logvar.shape)\n",
    "            loss = multiclass_vae_loss(recon, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    print('Epoch {}, Loss: {:.4f}'.format(epoch+1, total_loss / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the microbiome data and labels into PyTorch tensors\n",
    "data = torch.load(\"../Data/Tensors_objs/features.pt\")\n",
    "labels = torch.load(\"../Data/Tensors_objs/labels.pt\")\n",
    "data = torch.DoubleTensor(data)\n",
    "labels = labels.float()\n",
    "labels = labels.double()\n",
    "#labels = torch.DoubleTensor(labels)\n",
    "#nan_indices = torch.isnan(data)\n",
    "#data = torch.where(nan_indices, torch.tensor(0.0), data)\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(data, labels)\n",
    "# Define the dataset and dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([636, 150])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 151])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x151 and 15x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [385], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m VAE(\u001b[39m10\u001b[39m,\u001b[39m2\u001b[39m, \u001b[39m10\u001b[39m,\u001b[39m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train_multiclass_vae(model,dataloader,optimizer,\u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [367], line 83\u001b[0m, in \u001b[0;36mtrain_multiclass_vae\u001b[0;34m(model, train_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     82\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 83\u001b[0m     recon, mu, logvar \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mfloat(),   target\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     84\u001b[0m     \u001b[39mprint\u001b[39m(recon\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(mu\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [367], line 56\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[0;32m---> 56\u001b[0m     mu, logvar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x, y)\n\u001b[1;32m     57\u001b[0m     \u001b[39m#print(mu.shape)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[39m#print(logvar.shape)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(mu, logvar)\n",
      "Cell \u001b[0;32mIn [367], line 28\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Pass through encoder network\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     29\u001b[0m \u001b[39m#print(x.dtype)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m mu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc21(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x151 and 15x10)"
     ]
    }
   ],
   "source": [
    "model = VAE(10,2, 10,5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "train_multiclass_vae(model,dataloader,optimizer,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_size*2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_size)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=-1)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps*std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_hat, mu, log_var):\n",
    "    # Reconstruction error\n",
    "    recon_loss = F.mse_loss(x_hat, x, reduction='sum')\n",
    "\n",
    "    # KL divergence\n",
    "    kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = (recon_loss + kld_loss)\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [396], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m x \u001b[39m=\u001b[39m batch_data\n\u001b[0;32m---> 19\u001b[0m x_hat, mu, log_var \u001b[39m=\u001b[39m vae(x)\n\u001b[1;32m     21\u001b[0m loss \u001b[39m=\u001b[39m vae_loss(x, x_hat, mu, log_var)\n\u001b[1;32m     22\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [394], line 41\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 41\u001b[0m     mu, log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x)\n\u001b[1;32m     42\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(mu, log_var)\n\u001b[1;32m     43\u001b[0m     x_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z)\n",
      "Cell \u001b[0;32mIn [394], line 26\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 26\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     27\u001b[0m     mu, log_var \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mchunk(h, \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m mu, log_var\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "# Define the VAE model\n",
    "input_size = 16 # size of each input feature\n",
    "latent_size =2 # size of the latent variables\n",
    "vae = VAE(input_size, latent_size)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50# number of epochs\n",
    "batch_size =128 # size of each batch\n",
    "data_loader =  dataloader # torch.utils.data.DataLoader for your dataset\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch_data in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = batch_data\n",
    "        x_hat, mu, log_var = vae(x)\n",
    "\n",
    "        loss = vae_loss(x, x_hat, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, total_loss / len(data_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.cross_entropy(recon_x, x.reshape(-1, input_dim), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae1(model, eex, epochs, batch_size, learning_rate):\n",
    "    train_loader =  DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, eex in enumerate(train_loader):\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data.float())\n",
    "            loss = loss_function(recon_batch, data.float(), mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Epoch: {} \\t Loss: {:.6f}'.format(epoch+1, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 150 # Define the size of your input data\n",
    "my_list = 2\n",
    "z_dim = 150 # Define the size of the latent space\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(input_dim,my_list, z_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the microbiome data and labels into PyTorch tensors\n",
    "data = torch.load(\"../Data/Tensors_objs/features.pt\")\n",
    "labels = torch.load(\"../Data/Tensors_objs/labels.pt\")\n",
    "data = torch.DoubleTensor(data)\n",
    "labels = labels.float()\n",
    "labels = labels.double()\n",
    "#labels = torch.DoubleTensor(labels)\n",
    "nan_indices = torch.isnan(data)\n",
    "data = torch.where(nan_indices, torch.tensor(0.0), data)\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(data, labels)\n",
    "# Define the dataset and dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VAE.forward() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [386], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_vae1(model, dataloader, \u001b[39m50\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m1e-4\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [98], line 11\u001b[0m, in \u001b[0;36mtrain_vae1\u001b[0;34m(model, eex, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, eex \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m     recon_batch, mu, logvar \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     12\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(recon_batch, data\u001b[39m.\u001b[39mfloat(), mu, logvar)\n\u001b[1;32m     13\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: VAE.forward() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "train_vae1(model, dataloader, 50, 32, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VAE.state_dict(model), '../Model/vae_one_study2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load Model and encode data\n",
    "VAE.load_state_dict(model, torch.load('vae_one_study2.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     z, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode(data\u001b[39m.\u001b[39mfloat())\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z, _ = model.decode(data.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and classes needed in this example:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from itertools import product\n",
    "\n",
    "# Import Gaussian Naive Bayes classifier:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 0 2 1 0 1 4 4 1 1 2 0 4 0 3 1 3 1 1 1 1 3 1 3 3 4 0 4 1 0 3 4 2 1 1 1\n",
      " 1 2 1 3 1 3 4 2 1 3 1 2 2 3 0 3 3 0 1 1 0 4 1 1 1 1 1 3 1 1 0 1 4 3 1 1 1\n",
      " 1 1 1 2 4 1 0 1 4 4 4 4 2 1 1 1 1 0 3 1 4 1 1 4 4 1 1 0 3 4 4 1 1 4 0 1 1\n",
      " 1 2 1 4 1 0 3 4 3 4 3 0 4 3 2 1 4]\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into random train and test subsets:\n",
    "train, test, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=5)\n",
    "\n",
    "# Initialize classifier:\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier:\n",
    "classifier = gnb.fit(train, train_labels)\n",
    "# Make predictions with the classifier:\n",
    "predictive_labels = gnb.predict(test)\n",
    "print(predictive_labels)\n",
    "\n",
    "# Evaluate label (subsets) accuracy:\n",
    "print(accuracy_score(test_labels, predictive_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 4 3 4 4 3 4 0 1 1 1 1 4 1 1 0 0 0 2 4 1 1 4 1 1 1 1 1 1 1 1 2 4 1 4\n",
      " 1 3 0 1 1 0 0 4 0 3 3 1 2 1 3 1 1 1 4 1 1 0 1 0 1 1 0 1 1 0 1 0 4 0 4 1 2\n",
      " 4 1 1 3 1 1 4 1 1 1 1 4 3 4 1 3 4 3 0 2 3 4 2 3 2 1 1 1 1 2 0 1 1 4 1 1 1\n",
      " 1 3 3 1 1 4 0 1 3 1 1 1 1 2 1 1 1]\n",
      "0.9921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into random train and test subsets:\n",
    "train, test, train_labels, test_labels = train_test_split(compined, labels, test_size=0.2, random_state=11)\n",
    "\n",
    "# Initialize classifier:\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier:\n",
    "classifier = gnb.fit(train, train_labels)\n",
    "# Make predictions with the classifier:\n",
    "predictive_labels = gnb.predict(test)\n",
    "print(predictive_labels)\n",
    "\n",
    "# Evaluate label (subsets) accuracy:\n",
    "print(accuracy_score(test_labels, predictive_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(compined, labels)\n",
    "# Evaluate the classifier on the validation set\n",
    "y_val_pred = clf.predict(test)\n",
    "val_accuracy = accuracy_score(test_labels, y_val_pred)\n",
    "val_confusion_matrix = confusion_matrix(test_labels, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 1.0000\n",
      "Validation confusion matrix:\n",
      "[[17  0  0  0  0]\n",
      " [ 0 66  0  0  0]\n",
      " [ 0  0  9  0  0]\n",
      " [ 0  0  0 15  0]\n",
      " [ 0  0  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation confusion matrix:\\n{val_confusion_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries and data points\n",
    "x_min, x_max = test.min() - 0.1, test.max() + 0.1\n",
    "y_min, y_max = test.min() - 0.1, test.max() + 0.1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first, find the indices of the points in class A and class B\n",
    "indices_A = np.where(labels == 1)[0]\n",
    "indices_B = np.where(labels == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,\n",
       "        55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,\n",
       "        68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,\n",
       "        81,  82,  83,  84,  85,  86,  87,  88, 146, 147, 148, 149, 150,\n",
       "       151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163,\n",
       "       164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176,\n",
       "       177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
       "       190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202,\n",
       "       203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "       216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228,\n",
       "       229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
       "       242, 243, 244, 344, 345, 346, 347, 348, 352, 353, 354, 355, 357,\n",
       "       358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371,\n",
       "       372, 373, 374, 376, 377, 378, 379, 381, 382, 383, 384, 385, 386,\n",
       "       389, 390, 391, 392, 393, 397, 398, 399, 402, 404, 405, 406, 408,\n",
       "       410, 411, 412, 414, 415, 417, 420, 422, 423, 424, 461, 462, 463,\n",
       "       464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
       "       477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "       490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502,\n",
       "       503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515,\n",
       "       516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528,\n",
       "       529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541,\n",
       "       542])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data points (Tensors) in class A and class B\n",
    "points_A = data[indices_A]\n",
    "points_B = data[indices_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  closest point in class B to the first point in class A\n",
    "point_A = points_A[0]\n",
    "distances = torch.norm(points_B - point_A, dim=1)\n",
    "min_distance = torch.min(distances)\n",
    "closest_point_B = points_B[torch.argmin(distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum Euclidean distance between a point in class A and a point in class B is 24.572020023521226.\n",
      "The closest point in class B to the first point in class A is tensor([0.0000, 0.0000, 0.1819, 0.0000, 0.0000, 0.0000, 0.0000, 0.2239, 0.2333,\n",
      "        0.1362, 0.1235, 0.0000, 0.0000, 0.0000, 0.1215, 0.1320, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.1500, 0.0000, 0.0119, 0.0000, 0.0000, 0.0726,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2151, 0.0000, 0.0000, 0.1345,\n",
      "        0.0000, 0.1855, 0.0000, 0.1912, 0.0000, 0.0000, 0.2760, 0.0000, 0.0666,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2519, 0.0000, 0.0000, 1.2691,\n",
      "        0.0000, 0.0000, 0.0000, 2.5403, 0.0000, 1.4951, 1.4841, 1.2158, 3.4579,\n",
      "        0.0000, 0.2574, 1.5570, 0.0000, 3.4918, 1.1940, 0.0000, 0.0000, 0.1771,\n",
      "        0.0000, 0.0000, 1.9311, 1.8469, 4.2595, 2.4102, 1.9308, 0.0000, 1.5183,\n",
      "        4.4092, 0.0000, 0.4729, 0.0000, 0.2926, 2.7506, 0.0000, 0.0000, 2.1808,\n",
      "        6.7957, 3.5056, 2.4417, 0.0000, 0.0000, 0.1032, 0.8042, 0.0991, 0.1461,\n",
      "        3.3183, 0.0000, 1.9539, 1.0659, 0.0775, 0.0000, 0.4768, 0.0000, 0.0270,\n",
      "        0.0000, 1.5593, 3.4844, 1.9633, 0.3306, 0.0000, 0.0000, 0.0000, 1.8488,\n",
      "        0.0000, 1.3271, 0.0000, 0.0000, 0.0000, 0.0000, 1.9618, 0.0000, 3.0954,\n",
      "        1.7719, 0.0000, 2.6623, 1.8815, 0.0000, 0.0000, 0.1827, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.3736, 0.5692, 2.3507, 0.0347, 0.0000, 3.6566, 1.9310,\n",
      "        0.0000, 0.0000, 2.7296, 2.5002, 0.0000, 1.0050], dtype=torch.float64).\n",
      "The difference between the two points is tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -1.9172,  0.0000,  0.0000, -1.8660, -1.4439,  0.0000,\n",
      "         0.0000, -0.4773,  0.0000,  1.4951, -2.9613,  1.2158,  3.4579,  0.0000,\n",
      "         0.2574, -5.1283,  0.0000,  3.2428, -3.5884,  0.0000,  0.0000, -4.5995,\n",
      "         0.0000, -1.2142,  1.9311, -3.1258,  4.2595,  2.4102,  1.9308,  0.0000,\n",
      "        -5.2664,  4.4092,  0.0000,  0.4729,  0.0000, -6.5822,  2.7506,  0.0000,\n",
      "        -4.6088,  2.1808,  6.7957,  3.5056, -3.3284,  0.0000,  0.0000, -2.8530,\n",
      "        -3.6366, -5.2729, -6.1247,  3.1598,  0.0000,  1.8872,  0.9012, -3.4233,\n",
      "        -3.2590, -0.3176,  0.0000, -2.7533, -1.9591,  0.7810,  3.4391,  1.8413,\n",
      "        -1.6831,  0.0000,  0.0000, -2.4847,  1.8488,  0.0000,  1.1764,  0.0000,\n",
      "         0.0000,  0.0000, -1.2193,  1.9618,  0.0000,  2.6741,  1.7719,  0.0000,\n",
      "         2.5285,  0.9513,  0.0000, -1.8964, -2.0705,  0.0000, -1.3737,  0.0000,\n",
      "        -2.2585, -0.9377,  0.3989,  1.8909, -3.2630, -3.2157,  3.6566,  1.5836,\n",
      "         0.0000, -2.3542,  2.6934,  2.2065,  0.0000,  0.9973],\n",
      "       dtype=torch.float64).\n"
     ]
    }
   ],
   "source": [
    "# calculate the difference between the two points\n",
    "difference = closest_point_B - point_A\n",
    "\n",
    "print(f\"The minimum Euclidean distance between a point in class A and a point in class B is {min_distance}.\")\n",
    "print(f\"The closest point in class B to the first point in class A is {closest_point_B}.\")\n",
    "print(f\"The difference between the two points is {difference}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = torch.where(torch.all(data == closest_point_B, axis=1))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.9539, 1.0659, 0.0775, 0.0000, 0.4768, 0.0000, 0.0270, 0.0000,\n",
       "        1.5593, 3.4844, 1.9633, 0.3306, 0.0000, 0.0000, 0.0000, 1.8488, 0.0000,\n",
       "        1.3271, 0.0000, 0.0000, 0.0000, 0.0000, 1.9618, 0.0000, 3.0954, 1.7719,\n",
       "        0.0000, 2.6623, 1.8815, 0.0000, 0.0000, 0.1827, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.3736, 0.5692, 2.3507, 0.0347, 0.0000, 3.6566, 1.9310, 0.0000,\n",
       "        0.0000, 2.7296, 2.5002, 0.0000, 1.0050, 2.2519, 0.0000, 0.0000, 1.2691,\n",
       "        0.0000, 0.0000, 0.0000, 2.5403, 0.0000, 1.4951, 1.4841, 1.2158, 3.4579,\n",
       "        0.0000, 0.2574, 1.5570, 0.0000, 3.4918, 1.1940, 0.0000, 0.0000, 0.1771,\n",
       "        0.0000, 0.0000, 1.9311, 1.8469, 4.2595, 2.4102, 1.9308, 0.0000, 1.5183,\n",
       "        4.4092, 0.0000, 0.4729, 0.0000, 0.2926, 2.7506, 0.0000, 0.0000, 2.1808,\n",
       "        6.7957, 3.5056, 2.4417, 0.0000, 0.0000, 0.1032, 0.8042, 0.0991, 0.1461,\n",
       "        3.3183, 2.2519, 0.0000, 0.0000, 1.2691, 0.0000, 0.0000, 0.0000, 2.5403,\n",
       "        0.0000, 1.4951, 1.4841, 1.2158, 3.4579, 0.0000, 0.2574, 1.5570, 0.0000,\n",
       "        3.4918, 1.1940, 0.0000, 0.0000, 0.1771, 0.0000, 0.0000, 1.9311, 1.8469,\n",
       "        4.2595, 2.4102, 1.9308, 0.0000, 1.5183, 4.4092, 0.0000, 0.4729, 0.0000,\n",
       "        0.2926, 2.7506, 0.0000, 0.0000, 2.1808, 6.7957, 3.5056, 2.4417, 0.0000,\n",
       "        0.0000, 0.1032, 0.8042, 0.0991, 0.1461, 3.3183], dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wt/pl8gm0gs50d6vpk0nxfh07_h0000gn/T/ipykernel_20804/1601153969.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  single_tensor = torch.tensor(data[99])  # or any other single tensor\n"
     ]
    }
   ],
   "source": [
    "single_tensor = torch.tensor(data[99])  # or any other single tensor\n",
    "decoded_tensor = model.decode(single_tensor.unsqueeze(0).float())  # pass to the decoder of your VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# compute Euclidean distances between decoded tensor and all original tensors\n",
    "distances = cdist(decoded_tensor.detach().numpy(), data.numpy(), metric='euclidean')\n",
    "\n",
    "# find index of the original tensor with the smallest distance\n",
    "index = torch.argmin(torch.from_numpy(distances))\n",
    "\n",
    "# select the original tensor with the smallest distance\n",
    "original_sample = dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(465)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
